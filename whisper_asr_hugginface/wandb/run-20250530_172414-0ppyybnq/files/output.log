/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
                                                                                                                                   
{'loss': 2.1735, 'grad_norm': 19.782365798950195, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.05}
{'loss': 0.5866, 'grad_norm': 12.11807918548584, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.09}
{'loss': 0.3455, 'grad_norm': 11.008828163146973, 'learning_rate': 4.2e-05, 'epoch': 0.14}
{'loss': 0.3094, 'grad_norm': 8.347503662109375, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.19}
{'loss': 0.2978, 'grad_norm': 6.750919342041016, 'learning_rate': 7.199999999999999e-05, 'epoch': 0.24}
{'loss': 0.3035, 'grad_norm': 11.13472843170166, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.28}
{'loss': 0.2917, 'grad_norm': 7.629786491394043, 'learning_rate': 0.000102, 'epoch': 0.33}
{'loss': 0.281, 'grad_norm': 9.066434860229492, 'learning_rate': 0.000117, 'epoch': 0.38}
{'loss': 0.2893, 'grad_norm': 5.269966125488281, 'learning_rate': 0.00013199999999999998, 'epoch': 0.42}
{'loss': 0.2947, 'grad_norm': 10.218768119812012, 'learning_rate': 0.000147, 'epoch': 0.47}
{'loss': 0.2831, 'grad_norm': 9.556315422058105, 'learning_rate': 0.000162, 'epoch': 0.52}
{'loss': 0.2741, 'grad_norm': 6.65934419631958, 'learning_rate': 0.00017699999999999997, 'epoch': 0.57}
{'loss': 0.2992, 'grad_norm': 5.555295944213867, 'learning_rate': 0.00019199999999999998, 'epoch': 0.61}
{'loss': 0.2851, 'grad_norm': 7.987102031707764, 'learning_rate': 0.00020699999999999996, 'epoch': 0.66}
{'loss': 0.2898, 'grad_norm': 9.913646697998047, 'learning_rate': 0.00022199999999999998, 'epoch': 0.71}
{'loss': 0.2978, 'grad_norm': 16.67069435119629, 'learning_rate': 0.000237, 'epoch': 0.76}
{'loss': 0.2694, 'grad_norm': 6.317232131958008, 'learning_rate': 0.00025199999999999995, 'epoch': 0.8}
{'loss': 0.2767, 'grad_norm': 8.97496509552002, 'learning_rate': 0.000267, 'epoch': 0.85}
{'loss': 0.2607, 'grad_norm': 6.6534881591796875, 'learning_rate': 0.00028199999999999997, 'epoch': 0.9}
{'loss': 0.2995, 'grad_norm': 6.701059818267822, 'learning_rate': 0.00029699999999999996, 'epoch': 0.94}
{'loss': 0.2921, 'grad_norm': 5.863307952880859, 'learning_rate': 0.00029866666666666664, 'epoch': 0.99}
{'loss': 0.4664, 'grad_norm': 8.178797721862793, 'learning_rate': 0.00029699999999999996, 'epoch': 1.04}
{'loss': 0.2772, 'grad_norm': 7.303445339202881, 'learning_rate': 0.0002953333333333333, 'epoch': 1.09}
{'loss': 0.2661, 'grad_norm': 7.115750789642334, 'learning_rate': 0.00029366666666666663, 'epoch': 1.13}
{'loss': 0.2638, 'grad_norm': 6.694732666015625, 'learning_rate': 0.000292, 'epoch': 1.18}
{'loss': 0.3476, 'grad_norm': 3.7686171531677246, 'learning_rate': 0.0002903333333333333, 'epoch': 1.23}
{'loss': 0.2627, 'grad_norm': 10.781932830810547, 'learning_rate': 0.0002886666666666666, 'epoch': 1.27}
{'loss': 0.2378, 'grad_norm': 3.7906484603881836, 'learning_rate': 0.000287, 'epoch': 1.32}
{'loss': 0.2687, 'grad_norm': 4.572637557983398, 'learning_rate': 0.0002853333333333333, 'epoch': 1.37}
{'loss': 0.295, 'grad_norm': 5.789941310882568, 'learning_rate': 0.00028366666666666666, 'epoch': 1.42}
{'loss': 0.2424, 'grad_norm': 4.60429048538208, 'learning_rate': 0.00028199999999999997, 'epoch': 1.46}
{'loss': 0.2521, 'grad_norm': 8.209181785583496, 'learning_rate': 0.0002803333333333333, 'epoch': 1.51}
{'loss': 0.2422, 'grad_norm': 10.16634464263916, 'learning_rate': 0.00027866666666666665, 'epoch': 1.56}
{'loss': 0.2234, 'grad_norm': 6.924953937530518, 'learning_rate': 0.00027699999999999996, 'epoch': 1.61}
{'loss': 0.2254, 'grad_norm': 9.6025972366333, 'learning_rate': 0.0002753333333333333, 'epoch': 1.65}
{'loss': 0.2472, 'grad_norm': 4.007925510406494, 'learning_rate': 0.00027366666666666663, 'epoch': 1.7}
{'loss': 0.214, 'grad_norm': 4.710021018981934, 'learning_rate': 0.00027199999999999994, 'epoch': 1.75}
{'loss': 0.2313, 'grad_norm': 4.089413642883301, 'learning_rate': 0.0002703333333333333, 'epoch': 1.79}
{'loss': 0.2409, 'grad_norm': 6.857647895812988, 'learning_rate': 0.0002686666666666666, 'epoch': 1.84}
{'loss': 0.2076, 'grad_norm': 3.867759943008423, 'learning_rate': 0.000267, 'epoch': 1.89}
  File "/home/selinawisco/children_ssd_detection/whisper_asr_hugginface/0.multitask_whisper_asr_finetuning.py", line 237, in <module>
    trainer.train()
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer.py", line 2029, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer.py", line 2412, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 166, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer.py", line 3229, in evaluate
    output = eval_loop(
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/trainer.py", line 3440, in evaluation_loop
    logits = self.accelerator.pad_across_processes(logits, dim=1, pad_index=-100)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/accelerator.py", line 2507, in pad_across_processes
    return pad_across_processes(tensor, dim=dim, pad_index=pad_index, pad_first=pad_first)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 411, in wrapper
    return function(*args, **kwargs)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 678, in pad_across_processes
    return recursively_apply(
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 107, in recursively_apply
    return honor_type(
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 81, in honor_type
    return type(obj)(generator)
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 110, in <genexpr>
    recursively_apply(
  File "/home/selinawisco/anaconda3/envs/asr/lib/python3.10/site-packages/accelerate/utils/operations.py", line 128, in recursively_apply
    raise TypeError(
TypeError: Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
